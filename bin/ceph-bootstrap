#!/usr/bin/env bash

export PATH=/usr/local/bin:$PATH

eval `curl -fq http://169.254.169.254/latest/user-data`
REGION=`curl -sfq http://169.254.169.254/latest/dynamic/instance-identity/document | jq '.region' -r`

# This starts our Ceph Monitor
/usr/local/bin/nubis-ceph-bootstrap-mon

# At this point, the cluster of monitors will start assembling, but we
# need to wait until we've reached quorum before we proceed, otherwise
# we'd end up with monitors that have a colliding view of OSDs
# So, we wait until we see at least the same amount number of monitors
# as we were configured with

# Thinking about this, we should possibly just wait for positive quorom to be
# verified, this might help handle the situation when we lose a node...

MON_COUNT=`ceph mon dump -f json 2>/dev/null | jq '.mons | length' || echo -1`
while [ "${MON_COUNT:-0}" -lt "$NUBIS_STORAGE_CLUSTER_SIZE" ]; do
 MON_COUNT=`ceph mon dump -f json 2>/dev/null | jq '.mons | length' || echo -1`
 sleep 1
done

# Now start our Ceph OSD
/usr/local/bin/nubis-ceph-bootstrap-osd

echo "Waiting for cluster to report healthy"

while [ "$CEPH_HEALTH" != "HEALTH_OK" ]; do
  CEPH_HEALTH=$(ceph health)
  echo "Health reported is : $CEPH_HEALTH"
  if [ "$CEPH_HEALTH" != "HEALTH_OK" ]; then
    sleep 30
  fi
done

# Signal completion to CloudFormation
echo "Signaling CloudFormation stack $NUBIS_STORAGE_STACK_NAME we are done"
cfn-signal --stack=$NUBIS_STORAGE_STACK_NAME --region=$REGION --resource=AutoScalingGroup

